{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bayes.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "0GO4kAH_sjyz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Naive Bayes Classifier for Classifying Russian Troll Tweets\n",
        "---\n",
        "\n",
        "Bayes' theorem was introduced as a way to determine conditional probabilty for an event.\n",
        "\n",
        "This theorem has been used almost everywhere from finances to medicine. \n",
        "\n",
        "Bayes can be summed up as:\n",
        "\n",
        "![alt text](https://i.imgur.com/SnKmKwx.gif)\n",
        "\n",
        "Where P(A|B) is the probability of A being true given B, P(B|A) is the likelihood or the probability of A given B, P(A) is the probability of A being true before we look at our evidence (known as the prior term), and P(B) is the probability of our evidence being true (known as the evidence term).\n",
        "\n",
        "---\n",
        "\n",
        "Bayes' theorem has also seen use in machine learning through linear classifiers known as naive Bayes classifiers. \n",
        "\n",
        "These classifiers are called naive Bayes classifiers because they utilize Bayes' theorem, and make the assumption that every sample within our data is independent from other samples. \n",
        "\n",
        "Naive Bayes classifiers tend to hold up pretty well since they tend to perform really well given a limited dataset. \n",
        "\n",
        "---\n",
        "\n",
        "Utilizing a naive Bayes classifier, we will be going over how to tell if a tweet is from an alleged troll account.\n",
        "\n",
        "Lets get started.\n",
        "\n",
        "The first thing we need to do is install kaggle so that we can obtain our datasets."
      ]
    },
    {
      "metadata": {
        "id": "WXQcDvp9rYCX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wi3SOP4tse2t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once we have installed kaggle we need to upload our kaggle.json key and download the datasets."
      ]
    },
    {
      "metadata": {
        "id": "s_a2Tg0Sry6K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "79vZ0d4dq5g7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp kaggle.json ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d vikasg/russian-troll-tweets\n",
        "!kaggle datasets download -d shashank1558/preprocessed-twitter-tweets\n",
        "!kaggle datasets download -d speckledpingu/RawTwitterFeeds\n",
        "!unzip russian-troll-tweets.zip\n",
        "!unzip preprocessed-twitter-tweets.zip\n",
        "!unzip RawTwitterFeeds.zip\n",
        "!ls\n",
        "!rm kaggle.json\n",
        "!rm ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RCqOBkdiwL3l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With our datasets unpacked we should import the packages we will be using."
      ]
    },
    {
      "metadata": {
        "id": "48upEEyR-dlJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LPtfbwsMwPA_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our tweets consist of five different .csv files.\n",
        "\n",
        "Real tweets are contained within:\n",
        "- processedPositive.csv\n",
        "- processedNeutral.csv\n",
        "- processedNegative.csv\n",
        "- AllTweets.csv\n",
        "\n",
        "The fake troll tweets are within:\n",
        "- tweets.csv\n",
        "\n",
        "What we need to do is place all of these tweets in an array for our training set and an array for our test set. \n",
        "\n",
        "Each item within the arrays is labeled and represented as: [tweet text, label] where a label of 1 means the tweet is fake and a label of 0 means the tweet is real.\n"
      ]
    },
    {
      "metadata": {
        "id": "q7P-fbasfAte",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "a647d494-ced3-4993-88d7-2f99167847dc"
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "twts_pos = pd.read_csv('processedPositive.csv')\n",
        "twts_neut = pd.read_csv('processedNeutral.csv')\n",
        "twts_neg = pd.read_csv('processedNegative.csv')\n",
        "twts_various = pd.read_csv('AllTweets.csv')\n",
        "twts_various = twts_various.text.dropna()\n",
        "\n",
        "twts_real = pd.concat([twts_pos, twts_neut, twts_neg])\n",
        "twts_real_cnt = len(twts_real.columns) + len(twts_various)\n",
        "\n",
        "twts_fake = pd.read_csv('tweets.csv')\n",
        "twts_fake = twts_fake.text.dropna()\n",
        "\n",
        "# want to sample the data and make sure we have an even amount of data\n",
        "twts_fake_sample = twts_fake.sample(twts_real_cnt, random_state=0)\n",
        "\n",
        "labeled_tweets = []\n",
        "\n",
        "for row in tqdm(twts_fake_sample):\n",
        "  labeled_tweets.append([row, 1])\n",
        "  \n",
        "for row in tqdm(twts_real):\n",
        "  labeled_tweets.append([row, 0])\n",
        "\n",
        "for row in tqdm(twts_various):\n",
        "  labeled_tweets.append([row, 0])\n",
        "  \n",
        "labeled_tweets = np.array(labeled_tweets)\n",
        "np.random.shuffle(labeled_tweets)\n",
        "\n",
        "train_set_size = math.floor(len(labeled_tweets) * 0.2)\n",
        "\n",
        "test_set = labeled_tweets[:train_set_size]\n",
        "train_set = labeled_tweets[train_set_size:]\n",
        "\n",
        "print(train_set.shape)\n",
        "print(test_set.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 92497/92497 [00:00<00:00, 430570.67it/s]\n",
            "3872it [00:00, 894686.27it/s]\n",
            "100%|██████████| 88625/88625 [00:00<00:00, 418334.82it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(147996, 2)\n",
            "(36998, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g2I-yS_9wNfJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With the data split we have a training set of 147996 labeled tweets and a test set of 36998 labeled tweets.\n",
        "\n",
        "Now that we have created our training set and test set, we can start working on our naive Bayes classifier.\n",
        "\n",
        "We start off by creating two dictionaries that will store each word and the number of its occurances.\n",
        "We will then create a function for bagging each word of the tweet.\n",
        "\n",
        "Then we will create our training function that will iterate through every tweet in our training set, bag each word, and then compute our probability values for both fake and real tweets.\n"
      ]
    },
    {
      "metadata": {
        "id": "ddY6Z6Pwl68B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "578a7471-3cf7-4a0b-9360-5db674e0beaa"
      },
      "cell_type": "code",
      "source": [
        "false_tweets = {}\n",
        "real_tweets = {}\n",
        "\n",
        "def add_words_to_bag(words, label):\n",
        "  for word in words:\n",
        "    if label == '1':\n",
        "      false_tweets[word] = false_tweets.get(word, 0) + 1\n",
        "    else:\n",
        "      real_tweets[word] = real_tweets.get(word, 0) + 1\n",
        "      \n",
        "def train():\n",
        "  for tweet in train_set:\n",
        "    tweet_words = tweet[0].split()\n",
        "    add_words_to_bag(tweet_words, tweet[1])\n",
        "    total = (len(false_tweets) + len(real_tweets))\n",
        "    if total == 0:\n",
        "      total = 1\n",
        "  p_fake = len(false_tweets)/total\n",
        "  p_real = (total - len(false_tweets))/total\n",
        "  return p_real, p_fake\n",
        "\n",
        "p_real, p_fake = train()  \n",
        "print(p_real)\n",
        "print(p_fake)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.45905262596221624\n",
            "0.5409473740377837\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SkMsYXiRwQZb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After training we obtain a dictionaries filled with word occurances from each category and we obtain probabilities of the tweet being fake and it being real."
      ]
    },
    {
      "metadata": {
        "id": "jsKO54nF-ck5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tweet_probability(tweet, label):\n",
        "  probability = 1.0\n",
        "  tweet_words = tweet.split()\n",
        "  for word in tweet_words:\n",
        "    if label == 1:\n",
        "      probability *= (false_tweets.get(word, 0 ) + 1) / (len(false_tweets) + 1 * len(list(tweet_words)))\n",
        "    else:\n",
        "      probability *= (real_tweets.get(word, 0) + 1) / (len(real_tweets) + 1 * len(list(tweet_words)))\n",
        "  return probability\n",
        "\n",
        "def boolean_classify_tweet(tweet):\n",
        "  prob_real = p_real * tweet_probability(tweet, 0) \n",
        "  prob_fake = p_fake * tweet_probability(tweet, 1)\n",
        "  return prob_real > prob_fake\n",
        "\n",
        "def prob_classify_tweet(tweet):\n",
        "  prob_real = p_real * tweet_probability(tweet, 0)\n",
        "  prob_fake = p_fake * tweet_probability(tweet, 1)\n",
        "  return prob_real, prob_fake\n",
        "\n",
        "def test():\n",
        "  num_correct = 0\n",
        "  num_incorrect = 0\n",
        "  false_positives = 0\n",
        "  true_negatives = 0\n",
        "  for tweet in test_set:\n",
        "    classification = boolean_classify_tweet(tweet[0])\n",
        "    if((classification == True and tweet[1] == '0') or (classification == False and tweet[1] == '1')):\n",
        "      num_correct += 1\n",
        "    else:\n",
        "      num_incorrect += 1\n",
        "      if(tweet[1] == '1'):\n",
        "        false_positives += 1\n",
        "      else:\n",
        "        true_negatives += 1\n",
        "      \n",
        "  print('Correct: ' + str(num_correct))\n",
        "  print('Incorrect: ' + str(num_incorrect))\n",
        "  print('False positives: ' + str(false_positives))\n",
        "  print('True negatives: ' + str(true_negatives))\n",
        "  \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SOoptJNBwRf9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can now use the data obtained from training to classify tweets within our test set.\n",
        "\n",
        "After classifying all tweets, we can see the algorithm has obtained ~90% accuracy when classifying troll tweets."
      ]
    },
    {
      "metadata": {
        "id": "pNE49fcfAzyE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "f96a6c20-f218-43cc-d96d-ad247008e618"
      },
      "cell_type": "code",
      "source": [
        "test()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct: 33386\n",
            "Incorrect: 3612\n",
            "False positives: 3310\n",
            "True negatives: 302\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fn_kgL2gwSRD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This can be extended upon to improve the accuracy, but this works great as a proof of concept."
      ]
    }
  ]
}